{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of bbc-text-classification.ipynb","provenance":[{"file_id":"https://github.com/srushtidhope/bbc-text-classification/blob/master/bbc_text_classification.ipynb","timestamp":1591337163773}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"C2ZhK6jC9Nnv","colab_type":"code","outputId":"9f009920-eb25-4a9a-cd07-72779ea87993","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aoYqnsOI9gP-","colab_type":"code","outputId":"4d955dba-2cbf-4448-a23d-5d18029730c1","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1591337246280,"user_tz":-330,"elapsed":14251,"user":{"displayName":"gangisetti avinash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5ocbpJYVPYkpqk5pskP_V022muvHo7YtrBvJmqi8=s64","userId":"01582276880931399884"}}},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from gensim.models import Word2Vec\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ejIJG128-dVX","colab_type":"code","colab":{}},"source":["# Read data frame\n","drive_url = '/content/drive/My Drive/Colab Notebooks/bbc-text-classification/'\n","df = pd.read_csv(drive_url+'bbc-data.csv')\n","\n","# Description of the dataset\n","print('SHAPE OF DATASET: ', df.shape, '\\n\\nCOLUMNS IN DATASET: ', df.columns, '\\n\\nCATEGORIES: ', df.category.unique(), '\\n\\nDATA SAMPLE: \\n\\n', df.sample(n=5), '\\n\\n')\n","\n","# Plotting number of samples within each category\n","print('NUMBER OF SAMPLES IN EACH CATEGORY: \\n')\n","sns.countplot(df.category)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"22L7TrqYtFiz","colab_type":"code","outputId":"2988f642-699a-41bd-e255-f1417019b625","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["# DATA CLEANING\n","print('Data cleaning in progress...')\n","\n","# Tokenize\n","df['text_clean'] = df['text'].apply(nltk.word_tokenize)\n","print('Tokenization complete.')\n","\n","# Remove stop words\n","stop_words=set(nltk.corpus.stopwords.words(\"english\"))\n","df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if item not in stop_words])\n","print('Stop words removed.')\n","\n","# Remove numbers, punctuation and special characters (only keep words)\n","regex = '[a-z]+'\n","df['text_clean'] = df['text_clean'].apply(lambda x: [item for item in x if re.match(regex, item)])\n","print('Numbers, punctuation and special characters removed.')\n","\n","# Lemmatization\n","lem = nltk.stem.wordnet.WordNetLemmatizer()\n","df['text_clean'] = df['text_clean'].apply(lambda x: [lem.lemmatize(item, pos='v') for item in x])\n","print('Lemmatization complete.\\nData cleaning complete.\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Data cleaning in progress...\n","Tokenization complete.\n","Stop words removed.\n","Numbers, punctuation and special characters removed.\n","Lemmatization complete.\n","Data cleaning complete.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QiqczAO5OHM0","colab_type":"code","outputId":"b33714a9-337b-4a90-ae68-a61c95ee572e","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["# Classification using word2vec vectorizer\n","\n","vec_model = Word2Vec(df['text_clean'])\n","w2v = dict(zip(vec_model.wv.index2word, vec_model.wv.syn0))\n","\n","class Vectorizer(object):\n","    \n","    def __init__(self, vec):\n","        self.vec = vec\n","        self.dim = len(vec.values())\n","\n","    def fit(self, X, y):\n","        return self\n","\n","    def transform(self, X):\n","        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n","\n","class Classifier(object):\n","    \n","    def __init__(self, model, param):\n","        self.model = model\n","        self.param = param\n","        self.gs = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)        \n","\n","    def fit(self, X, y):        \n","        return self.gs.fit(X, y)\n","\n","    def predict(self, X):\n","        return self.gs.predict(X)\n","\n","clf_models = {\n","    'Naive Bayes': GaussianNB(), \n","    'SVC': SVC(),\n","    'Decision Tree': DecisionTreeClassifier(),  \n","    'Perceptron': MLPClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier()\n","}\n","\n","clf_params = {\n","    'Naive Bayes': { }, \n","    'SVC': { 'kernel': ['linear', 'rbf'] },\n","    'Decision Tree': { 'min_samples_split': [2, 5] }, \n","    'Perceptron': { 'activation': ['tanh', 'relu'] },\n","    'Gradient Boosting': { 'min_samples_split': [2, 5] }\n","}\n","\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n","\n","for key in clf_models.keys():\n","    \n","    clf = Pipeline([('Word2Vec vectorizer', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n","    \n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    \n","    print(key, ':')\n","    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Naive Bayes :\n","Accuracy: 0.348 \tPrecision: 0.373 \tRecall: 0.329 \t\tF1: 0.312\n","\n","SVC :\n","Accuracy: 0.294 \tPrecision: 0.119 \tRecall: 0.264 \t\tF1: 0.163\n","\n","Decision Tree :\n","Accuracy: 0.353 \tPrecision: 0.351 \tRecall: 0.351 \t\tF1: 0.350\n","\n","Perceptron :\n","Accuracy: 0.303 \tPrecision: 0.134 \tRecall: 0.273 \t\tF1: 0.172\n","\n","Gradient Boosting :\n","Accuracy: 0.425 \tPrecision: 0.416 \tRecall: 0.415 \t\tF1: 0.412\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YTfHetcHmoaM","colab_type":"code","outputId":"9c1a2a1c-2c91-4df3-d0f2-880f1faefcb1","colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["# Classification using TFIDF vectorizer\n","\n","# Vectorize training and testing data\n","def Vectorize(vec, X_train, X_test):    \n","    \n","    X_train_vec = vec.fit_transform(X_train)\n","    X_test_vec = vec.transform(X_test)\n","    \n","    print('Vectorization complete.\\n')\n","    \n","    return X_train_vec, X_test_vec\n","\n","# Use multiple classifiers and grid search for prediction\n","def ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n","    \n","    if not set(models.keys()).issubset(set(params.keys())):\n","        raise ValueError('Some estimators are missing parameters')\n","\n","    for key in models.keys():\n","    \n","        model = models[key]\n","        param = params[key]\n","        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n","        gs.fit(X_train, y_train)\n","        y_pred = gs.predict(X_test)\n","        \n","        # Print scores for the classifier\n","        print(key, ':', gs.best_params_)\n","        print(\"Precision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n","    \n","    return\n","\n","models = {\n","    'Naive Bayes': MultinomialNB(), \n","    'Decision Tree': DecisionTreeClassifier(),  \n","    'Perceptron': MLPClassifier(),\n","    'Gradient Boosting': GradientBoostingClassifier()\n","}\n","\n","params = {\n","    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] }, \n","    'Decision Tree': { 'min_samples_split': [1, 2, 5] }, \n","    'Perceptron': { 'alpha': [0.0001, 0.001], 'activation': ['tanh', 'relu'] },\n","    'Gradient Boosting': { 'learning_rate': [0.05, 0.1], 'min_samples_split': [2, 5] }\n","}\n","\n","# Encode label categories to numbers\n","enc = LabelEncoder()\n","df['category'] = enc.fit_transform(df['category'])\n","labels = list(enc.classes_)\n","\n","# Train-test split and vectorize\n","X_train, X_test, y_train, y_test = train_test_split(df['text'], df['category'], test_size=0.2, shuffle=True)\n","X_train_vec, X_test_vec = Vectorize(TfidfVectorizer(), X_train, X_test)\n","\n","ML_modeling(models, params, X_train_vec, X_test_vec, y_train, y_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vectorization complete.\n","\n","Naive Bayes : {'alpha': 0.5, 'fit_prior': False}\n","Precision: 0.940 \tRecall: 0.943 \t\tF1: 0.939\n","\n","Decision Tree : {'min_samples_split': 2}\n","Precision: 0.806 \tRecall: 0.808 \t\tF1: 0.806\n","\n","Perceptron : {'activation': 'relu', 'alpha': 0.0001}\n","Precision: 0.969 \tRecall: 0.971 \t\tF1: 0.970\n","\n","Gradient Boosting : {'learning_rate': 0.1, 'min_samples_split': 2}\n","Precision: 0.954 \tRecall: 0.954 \t\tF1: 0.953\n","\n"],"name":"stdout"}]}]}